Machine Learning
----------------
Sessie 7 is optioneel, opdracht ook niet indienen.
Punten op 3 van de 6 opdrachten, geen examen.

01 Regressie
------------

- enkelvoudige lineaire regressie (-> Scatterplot)
	y = b1*x + b0
	R² : determinatiecoef. -> hoe goed kan model op basis van verandering van X de verandering van Y voorspellen.
	
Meervoudige lineair regressie

- meerdere features -> voorspelling

1) consistentie van de dataset
	volledigheid controleren (rijen/kolommen)
		#python
		dataset.describe()
	dummy variabelen droppen
	uitschieters
		#python
		from scipy import stats
		dataset = dataset[(np.abs(stats.zscore(dataset)) < 3).all(axis=1)]
		dataset.describe()
	onderline correlatie
		heatmap		
			#python
			f, ax = plt.subplots(figsize=(10, 8))
			corr = dataset.corr()
			sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
						square=True, ax=ax,annot=True)
		correlatie afprinten
			#python
			dataset.corr()

2) Dataset opsplitsen in features (X) en targets (y)
	y = dataset['target'].values()
	X = dataset.drop

3) dataset opsplitsen in training en test set
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)

4) trainen van het model
	lregmodel = linear_model.LinearRegression()
	lregmodel.fit(X_train, y_train)

5) testen van het model
	X_test, y_test

	MAE (Mean Absolute Error) (gemakkelijker interpreteerbaar)
	MAE = 1/n sum(abs(y-ypred))

		#python
		from sklearn.metrics import mean_absolute_error
		y_predicted = lregmodel.predict(X_test)
		mae = mean_absolute_error(y_test, y_predicted)
	
	MSE (Mean Squared Error) (benadrukt grote uitschieters)
	MSE = 1/n sum((y-ypred)**2)
		#python
		from sklearn.metrics import mean_squared_error
		y_predicted = lregmodel.predict(X_test)
		mse = mean_squared_error(y_test, y_predicted)

	R² Determininatiecoëfficiënt
	R² = 1 - sum((y-ypred)**2)/sum((y-ybar)**2)
		perfecte voorspelling -> R² = 1
		R² = 0 -> horizontale lijn
		(superslecht -> -oneindig)
		#python
		from sklearn.metrics import r2_score
		y_predicted = lregmodel.predict(X_test)
		r2 = r2_score(y_test, y_predicted)


6) Modeloptimalisatie en feature engineering
	hoe optimaliseren?
		- meer data 
			-> meer rijen
			-> meer features
		- feature engineering & expansion
		- ander model
		- hyperparameter tuning

	Normalisatie / SCALING
		zorgt ervoor dat alle features op dezelfde schaalverdeling staan
		-> model zal sneller trainen (meestal).
		-> theta vergelijkbaar maken
		-> Regularisatie

		min-max scaling
			-gevoelig aan outliers
		standardscaling (normalisatie)
			-kan beter overweg met uitschieters
			-vervormt geen relatieve afstanden tss feature waarden
		Robust scaling
			-ipv min gebruiken we de interkwartielafstand.


	Feature expansion
		- Bedenken van nieuwe features
			* lengte * breedte -> oppervlakte
			* start en eind -> afstanden
			* datum -> dag van de week
			* verandering in de features
			* nieuwe opgemeten parameters.

		- Hogere orde features
			P = theta0 + theta1*x + theta2*x**2
			#python
			graad = 3
			poly = PolynomialFeatures(graad)
			poly.fit(X_train)
			X_train_poly = poly.transform(X_train)
			X_test_poly = poly.transform(X_test)
		
		-One-hot encoding
			omzetten van categorische variabelen naar meerdere aparte features
			#python
			dataset = pd.concat([dataset,pd.get_dummies(dataset['food_name'], prefix='food')],axis=1)
			dataset.drop(['food_name'],axis=1, inplace=True)
			


	Underfitting en overfitting
		- Underfitting -> hogere macht ontbreekt waardoor niet correct kan gefit worden (te simpel)
		- Overfitting -> moet exact hetzelfde zijn als trainingset vooraleer het werkt. (te complex)


					

	



